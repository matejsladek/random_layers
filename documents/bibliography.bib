@inproceedings{Schmidt_Kraaijveld_Duin, 
	title={Feedforward neural networks with random weights}, url={http://dx.doi.org/10.1109/ICPR.1992.201708}, DOI={10.1109/icpr.1992.201708},
	booktitle={Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems}, 
	publisher={IEEE Comput. Soc. Press},
	author={Schmidt, W.F. and Kraaijveld, M.A. and Duin, R.P.W.}
}
@article{DBLP:journals/corr/CanzianiPC16,
	author    = {Alfredo Canziani and
		Adam Paszke and
		Eugenio Culurciello},
	title     = {An Analysis of Deep Neural Network Models for Practical Applications},
	journal   = {CoRR},
	volume    = {abs/1605.07678},
	year      = {2016},
	url       = {http://arxiv.org/abs/1605.07678},
	archivePrefix = {arXiv},
	eprint    = {1605.07678},
	timestamp = {Mon, 13 Aug 2018 16:46:15 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/CanzianiPC16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Li_Shen_Dou_Ni_Xu_Yang_Wang_Niu_2019, 
	title={A Novel Memory-Scheduling Strategy for Large Convolutional Neural Network on Memory-Limited Devices}, 
	volume={2019}, url={http://dx.doi.org/10.1155/2019/4328653}, DOI={10.1155/2019/4328653}, 
	abstractNote={<jats:p>Recently, machine learning, especially deep learning, has been a core algorithm to be widely used in many fields such as natural language processing, speech recognition, object recognition, and so on. At the same time, another trend is that more and more applications are moved to wearable and mobile devices. However, traditional deep learning methods such as convolutional neural network (CNN) and its variants consume a lot of memory resources. In this case, these powerful deep learning methods are difficult to apply on mobile memory-limited platforms. In order to solve this problem, we present a novel memory-management strategy called mmCNN in this paper. With the help of this method, we can easily deploy a trained large-size CNN on any memory size platform such as GPU, FPGA, or memory-limited mobile devices. In our experiments, we run a feed-forward CNN process in some extremely small memory sizes (as low as 5 MB) on a GPU platform. The result shows that our method saves more than 98% memory compared to a traditional CNN algorithm and further saves more than 90% compared to the state-of-the-art related work “vDNNs” (virtualized deep neural networks). Our work in this paper improves the computing scalability of lightweight applications and breaks the memory bottleneck of using deep learning method on memory-limited devices.</jats:p>}, 
	journal={Computational Intelligence and Neuroscience}, 
	publisher={Hindawi Limited}, 
	author={Li, Shijie and Shen, Xiaolong and Dou, Yong and Ni, Shice and Xu, Jinwei and Yang, Ke and Wang, Qiang and Niu, Xin}, 
	year={2019}, 
	month={Apr}, 
	pages={1–12}
}
@article{DBLP:journals/corr/GruslysMDLG16,
	author    = {Audrunas Gruslys and
	R{\'{e}}mi Munos and
	Ivo Danihelka and
	Marc Lanctot and
	Alex Graves},
	title     = {Memory-Efficient Backpropagation Through Time},
	journal   = {CoRR},
	volume    = {abs/1606.03401},
	year      = {2016},
	url       = {http://arxiv.org/abs/1606.03401},
	archivePrefix = {arXiv},
	eprint    = {1606.03401},
	timestamp = {Mon, 13 Aug 2018 16:48:49 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/GruslysMDLG16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Goldreich_Goldwasser_Micali_1986, 
	title={How to construct random functions},
	volume={33}, 
	url={http://dx.doi.org/10.1145/6490.6503}, 
	DOI={10.1145/6490.6503}, 
	number={4}, 
	journal={Journal of the ACM}, 
	publisher={Association for Computing Machinery (ACM)}, 
	author={Goldreich, Oded and Goldwasser, Shafi and Micali, Silvio}, 
	year={1986}, 
	month={Aug}, 
	pages={792–807} 
}
@misc{chen2016training,
	title={Training Deep Nets with Sublinear Memory Cost},
	author={Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
	year={2016},
	eprint={1604.06174},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@inproceedings{DBLP:conf/icml/DiamosSCCCEEHS16,
	author={Greg Diamos and Shubho Sengupta and Bryan Catanzaro and Mike Chrzanowski and Adam Coates and Erich Elsen and Jesse Engel and Awni Y. Hannun and Sanjeev Satheesh},
	title={Persistent RNNs: Stashing Recurrent Weights On-Chip},
	year={2016},
	cdate={1451606400000},
	pages={2024-2033},
	url={http://proceedings.mlr.press/v48/diamos16.html},
	booktitle={ICML},
	crossref={conf/icml/2016}
}
