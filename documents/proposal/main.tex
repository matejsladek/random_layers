\documentclass[]{article}

\usepackage[a4paper, margin=2cm]{geometry}
\usepackage{multicol}
\usepackage{cite}
\usepackage{hyperref}
% TODO: Remove import
\usepackage{blindtext}

%opening
\title{\vspace{-2cm}Deep Learning Fall 2019 Project Proposal\\\large{Large, Random Non-trainable Layers in Neural Networks with Memory Constraints}}
\author{Matej Sladek \& Bram van den Heuvel}

\pagestyle{empty}

\begin{document}
\maketitle

\begin{multicols}{2}
	\section{Introduction}
	Classical statistics suggests there is a paradox going on in deep learning: the models have an enormous number of parameters, but they don't suffer from the downsides of that fact the same way that virtually all other model classes do.
	
	In the early 1990's, Schmidt et. al. \cite{Schmidt_Kraaijveld_Duin} attempted to explain this phenomena by showing that many of the parameters in a deep model are of less importance. They describe how the first layers of a neural net can be viewed as a collection of nonlinear features of the input data, with a simple linear regression attached to it in the final layer. The weights in the final layer are of higher importance. They demonstrate this by randomly initializing some of the hidden layers with random weights and keeping their values fixed throughout the training procedure. We call these layers \emph{random layers}. The training procedure still works surprisingly well.
	
	
	\section{Depth and memory usage in state-of-the-art models}
	The value of depth and a large number of parameters is demonstrated by the models performing best in the ImageNet Large Scale Visual Recognition Challenge (INLSVRC) \cite{DBLP:journals/corr/CanzianiPC16}. In the last decade, the number of layers in the top-performing model has gone up to 152 with ResNet. Also the number parameters has grown; models often use tens of millions of parameters, VGG-19 as much as 155 million. This translates to a maximum memory usage during training of approximately 200 MB for most models, a little over 800 MB for VGG-19, when trained with a batch size of one. This then increases to above 1000 MB for several top-performing models once the batch is increased to 16, and increases approximately linearly with batch size beyond that. This means that with a batch size of 64, none of the top performing models can be trained on a mobile device or a consumer-grade GPU because of memory limitations.
	
	\section{Idea}
	We attempt to show improvement in the performance, memory usage and training time trade-off in deep neural networks. By replacing existing layers or introducing new layers, that are potentially very wide, random and non-trainable, we achieve depth. We think to have found a way to do so without incurring noticable memory usage by applying a "trick" inspired by pseudo-random functions \cite{Goldreich_Goldwasser_Micali_1986}. Activations would need to be recomputed at each pass, but due to the specifics of our trick, we expect it to be cheaper than in, for example, \cite{DBLP:journals/corr/GruslysMDLG16} as memory bandwidth can be a bottleneck \cite{Li_Shen_Dou_Ni_Xu_Yang_Wang_Niu_2019}.
	
	\section{Methods}
	We study the effect of introducing new random layers, or replacing existing layers by random layers on performance and memory usage. We don't have the means to provide an implementation of our idea. We will be able to measure model performance exactly by simply storing the random layers completely. Memory usage will have to be estimated, and the effects on training time we can most likely only analyze qualitatively.
	
	\subsection{Performance}
	To measure the effect of random layers on performance we identified the following tasks:
	\begin{itemize}
		\itemsep0em
		\item Convolutional architecture to classify images from the MNIST dataset
		\item Dense architecture to classify on the higgs dataset
		\item Dense architecture to do regression on the Seattle Airbnb dataset
	\end{itemize}
	The models we identified are as follows:
	\begin{itemize}
		\itemsep0em
		\item A small model, slightly too simple to perform well
		\item A large model, performing well
	\end{itemize}
	For each task-model combination we will evaluate a baseline model, the baseline with a layer replaced, and the baseline with a layer added.
	
	If we find improvement, we will report how many layers, and of which size, works best for the selected tasks.
	
	If time permits, we will attempt to improve one of the INLSVRC models.
	
	\subsection{Memory usage}
	We will attempt to measure or approximate by calculation the maximum memory usage throughout the training for the baseline models, and then calculate the memory usage of the other variants relative to that of the baseline model.
	
	\subsection{Training time}
	We describe in detail, in a qualitative manner, the ways in which the application of the trick might influence training time.
	
	\bibliography{../bibliography}
	\bibliographystyle{plain}
\end{multicols}
\end{document}
