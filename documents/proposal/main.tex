\documentclass[]{article}

\usepackage{geometry}
\usepackage{blindtext}

%opening
\title{Deep Learning Fall 2019 Project Proposal\\\large{Large, Random Non-trainable Layers in Neural Networks with Memory Constraints}}
\author{Matej Sladek \& Bram van den Heuvel}

\begin{document}
\maketitle

\section{Introduction}
Classical statistics suggests there is a paradox going on in deep learning: the models have an enormous number of parameters, but they don't suffer from the downsides of that fact the same way that virtual all other model classes do.

In the early 1990's, a paper by Schmidt et. al. attempts to explain this phenomena by showing that many of the parameters in a deep model are of less importance. They describe how the first layers of a neural net can be viewed as a collection of nonlinear features of the input data, with a simple linear regression attached to it in the final layer. The weights in the final layer are of higher importance. They demonstrate this by randomly initializing some of the hidden layers with random weights, and then keep their values fixed throughout the training procedure. Fixing these values reduced performance only slightly in their experiments.


\section{Depth and Memory limitations in state-of-the-art models}
The value of depth and a large number of parameters is demonstrated by the models performing best in the ImageNet Large Scale Visual Recognition Challenge. In the last decade, the number of layers in the top-performing model has gone up to 152 with ResNet. Also the number parameters has grown; models often use tens of millions of parameters, VGG-19 as much as 155 million.

This translates to a maximum memory usage of approximately 200 MB for most models, a little over 800 MB for VGG-19, when trained with a batch size of one. This then increases to above 1000 MB for several top-performing models once the batch is increased to 16, and increases approximately linearly with batch size beyond that. This means that with a batch size of 64, none of the top performing models can be trained on a consumer-grade GPU.

\section{Proposal}
We hope to improve on the memory usage / training time / performance trade-off in neural network models by replacing existing layers or introducing new layers that are large, random and non-trainable and who's weights are regenerated from the same seed at each pass in a parallel manner, analogous to pseudo-random functions in cryptography. This will provide depth while not increasing the memory usage; activations are recomputed at each pass within the core, as to not saturate the floating point datapath and as such minimizing the activation recomputation time penalty.

\end{document}
